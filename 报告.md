# 训练曲线分析报告（W&B: kjust-pinduoduo/lerobot/64wxlc3y）

## 概览
- 历史条目: 1200
- 训练指标: train/loss, train/l1_loss, train/kld_loss, train/ot_loss, train/ot_ot_loss, train/ot_ot_pi_sum, train/ot_ot_pi_diag, train/ot_ot_cost/observation.state, train/grad_norm, train/lr, train/update_s, train/dataloading_s
- 评估指标: eval/offline_eval/avg_loss, eval/offline_eval/avg_l1, eval/offline_eval/n_batches

## 关键指标趋势（训练）
- 总损失 `train/loss`: 首值=15.12, 末值=0.05973, 最优=0.05837 @ step=19850, Δ=-15.06 (-99.60%)
- L1 动作损失 `train/l1_loss`: 首值=0.5182, 末值=0.05082, 最优=0.04248 @ step=16500, Δ=-0.4673 (-90.19%)
- KL 损失 `train/kld_loss`: 首值=0.4484, 末值=0.0006399, 最优=0.0004342 @ step=19050, Δ=-0.4478 (-99.86%)
- OT 损失 `train/ot_loss`: 首值=0, 末值=1.721e-12, 最优=0 @ step=50, Δ=nan (nan%)
- OT(模块)损失 `train/ot_ot_loss`: 首值=0, 末值=1.721e-12, 最优=0 @ step=50, Δ=nan (nan%)
- OT π 总和 `train/ot_ot_pi_sum`: 首值=0, 末值=7.302e-12, 最优=0 @ step=50, Δ=nan (nan%)
- OT π 对角和 `train/ot_ot_pi_diag`: 首值=0, 末值=7.302e-12, 最优=0 @ step=50, Δ=nan (nan%)
- OT term(observation.state) 平均代价 `train/ot_ot_cost/observation.state`: 首值=1496, 末值=1260, 最优=309.6 @ step=7000, Δ=-235.3 (-15.73%)
- 梯度范数 `train/grad_norm`: 首值=249.7, 末值=7.024, 最优=6.475 @ step=19250, Δ=-242.6 (-97.19%)
- 学习率 `train/lr`: 首值=1e-05, 末值=1e-05, 最优=1e-05 @ step=50, Δ=0 (0.00%)
- 更新耗时(s) `train/update_s`: 首值=0.1887, 末值=0.1051, 最优=0.1026 @ step=15550, Δ=-0.08368 (-44.34%)
- 取数耗时(s) `train/dataloading_s`: 首值=0.2162, 末值=0.1504, 最优=0.104 @ step=7950, Δ=-0.06577 (-30.42%)

## 关键指标（离线评估）
- 离线评估/平均损失 `eval/offline_eval/avg_loss`: 首值=0.658, 末值=0.3222, 最优=0.303 @ step=4750
- 离线评估/平均L1 `eval/offline_eval/avg_l1`: 首值=0.658, 末值=0.3222, 最优=0.303 @ step=4750
- 离线评估/批次数 `eval/offline_eval/n_batches`: 首值=20, 末值=20, 最优=20 @ step=50

## 诊断与解读
- 总损失显著下降(-99.6%)，训练在收敛。
- L1损失显著下降(-90.2%)，训练在收敛。
- π 总和末值 7.302e-12；unbalanced OT 下不必等于1，需结合 reg / tau 观测数值稳定性。
- π 对角和末值 7.302e-12；越大意味着匹配更靠近对角（时间一致性更强）。

## 建议与后续
- 若训练损失进入平台期：尝试微调 reg（增大更平滑、稳定），或调整 lambda_ot 平衡 BC 与 OT。
- 如对角和偏低：适当提高 tau_src/tau_tgt 或开启 heuristic 对角先验（已在配置中可选）。
- 关注梯度范数与学习率：若梯度波动大或爆炸，建议减小 lr 或更严格的 grad_clip。
- 数据与更新耗时：若 data_s 高于 update_s，考虑增大 DataLoader workers、预取或存储优化。